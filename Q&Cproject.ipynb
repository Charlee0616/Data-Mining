{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhUeEIrzZujKuVan8uQTjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charlee0616/Data-Mining/blob/main/Q%26Cproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YzFcY8DQlt-8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Attribute_selection_method(dataset, target, method, classification):\n",
        "  if(method == \"classification\"): #method chosen was classification\n",
        "    features = dataset.columns[dataset.columns != target]\n",
        "    entropys=[]\n",
        "    for attribute in features:\n",
        "      entropy = 0\n",
        "      overall = len( dataset )\n",
        "      if(classification ==\"entropy\"): #classifcation was chosen and entropy was chosen\n",
        "        for val in vals:\n",
        "          if(len(dataset[attribute].unique())<20):\n",
        "            subset_size = len(dataset[ dataset[attribute] == val ])\n",
        "            weight = subset_size / overall\n",
        "            props = dataset[ dataset[attribute] == val ][target].value_counts( normalize=True )\n",
        "            for p in props.array:\n",
        "              entropy =  entropy - weight*(p*math.log2(p))\n",
        "        else: #classification was chose and Gini was chosen\n",
        "          entropy = 0\n",
        "          left = dataset[ dataset[attribute] <= val ][ [attribute,target] ]\n",
        "          props = left[ target ].value_counts( normalize = True )\n",
        "          weight = len( left ) / overall\n",
        "          for prop in props.array:\n",
        "            entropy = entropy - weight*prop*math.log2( prop )\n",
        "          right = dataset[ dataset[attribute] > val ][ [attribute,target] ]\n",
        "          props = right[ target ].value_counts( normalize = True )\n",
        "          weight = len( right ) / overall\n",
        "          for prop in props.array:\n",
        "            entropy = entropy - weight*prop*math.log2( prop )\n",
        "          entropys.append(entropy)\n",
        "      else:\n",
        "        entropy = 1 - sum([p ** 2 for p in props])\n",
        "        entropy += weight * entropy\n",
        "        entropys.append(entropy)\n",
        "    minEnt=min(entropys)\n",
        "    ind=entropys.index(minEnt)\n",
        "    return features[ind]\n",
        "  else:\n",
        "    # summarize = separate_by_class(dataset)\n",
        "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "    del(summaries[-1])\n",
        "    separated = dict()\n",
        "    for i in range(len(dataset)):\n",
        "      vector = dataset[i]\n",
        "    class_value = vector[-1]\n",
        "    if (class_value not in separated):\n",
        "      separated[class_value] = list()\n",
        "    separated[class_value].append(vector)\n",
        "    summarize = separated\n",
        "    #\n",
        "    predictions = list()\n",
        "    #put test into the method\n",
        "    for row in test:\n",
        "      #output = predict(summarize, row)\n",
        "      #probabilities = calculate_class_probabilities(summaries, row)\n",
        "        total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "        probabilityd = dict()\n",
        "        for class_value, class_summaries in summaries.items():\n",
        "          probabilityd[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
        "        for i in range(len(class_summaries)):\n",
        "          mean, stdev, _ = class_summaries[i]\n",
        "          #probabilityd[class_value] *= calculate_probability(row[i], mean, stdev)\n",
        "          exponent = exp(-((row[i]-mean)**2 / (2 * stdev**2 )))\n",
        "          probabilityd[class_value] *=(1 / (sqrt(2 * pi) * stdev)) * exponent\n",
        "          #\n",
        "        probabilities=probabilityd\n",
        "      #\n",
        "        best_label, best_prob = None, -1\n",
        "        for class_value, probability in probabilities.items():\n",
        "          if best_label is None or probability > best_prob:\n",
        "            best_prob = probability\n",
        "            best_label = class_value\n",
        "        output=best_label\n",
        "      #\n",
        "        predictions.append(output)\n",
        "    return(predictions)"
      ],
      "metadata": {
        "id": "ef6qrqhx0CcX"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}