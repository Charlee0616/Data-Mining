{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNWkMND6PHmCv1x8pTv+B+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charlee0616/Data-Mining/blob/main/Q%26Cproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YzFcY8DQlt-8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Attribute_selection_method(dataset, target, method, classification):\n",
        "  if(method == \"classification\")\n",
        "    features = dataset.columns[dataset.columns != target]\n",
        "    entropys=[]\n",
        "    for attribute in features:\n",
        "      entropy = 0\n",
        "      overall = len( dataset )\n",
        "      if(classification ==\"entropy\"):\n",
        "        for val in vals:\n",
        "          if(len(dataset[attribute].unique())<20):\n",
        "            subset_size = len(dataset[ dataset[attribute] == val ])\n",
        "            weight = subset_size / overall\n",
        "            props = dataset[ dataset[attribute] == val ][target].value_counts( normalize=True )\n",
        "            for p in props.array:\n",
        "              entropy =  entropy - weight*(p*math.log2(p))\n",
        "        else:\n",
        "          entropy = 0\n",
        "          left = dataset[ dataset[attribute] <= val ][ [attribute,target] ]\n",
        "          props = left[ target ].value_counts( normalize = True )\n",
        "          weight = len( left ) / overall\n",
        "          for prop in props.array:\n",
        "            entropy = entropy - weight*prop*math.log2( prop )\n",
        "          right = dataset[ dataset[attribute] > val ][ [attribute,target] ]\n",
        "          props = right[ target ].value_counts( normalize = True )\n",
        "          weight = len( right ) / overall\n",
        "          for prop in props.array:\n",
        "            entropy = entropy - weight*prop*math.log2( prop )\n",
        "          entropys.append(entropy)\n",
        "      else:\n",
        "        entropy = 1 - sum([p ** 2 for p in props])\n",
        "        entropy += weight * entropy\n",
        "        entropys.append(entropy)\n",
        "    minEnt=min(entropys)\n",
        "    ind=entropys.index(minEnt)\n",
        "    return features[ind]"
      ],
      "metadata": {
        "id": "ef6qrqhx0CcX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}