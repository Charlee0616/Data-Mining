{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPttummAp0/h24pD/jhK0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charlee0616/Data-Mining/blob/main/Q%26Cproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YzFcY8DQlt-8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statistics import mean, stdev"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Attribute_selection_method(dataset, target, method, classification):\n",
        "  if(method == \"classification\"): #method chosen was classification\n",
        "    features = dataset.columns[dataset.columns != target]\n",
        "    entropys=[]\n",
        "    for attribute in features:\n",
        "      entropy = 0\n",
        "      overall = len( dataset )\n",
        "      #vals = dataset[attribute].unique()\n",
        "      if(classification ==\"entropy\"): #classifcation was chosen and entropy was chosen\n",
        "        for val in vals:\n",
        "          if(len(dataset[attribute].unique())<20):\n",
        "            subset_size = len(dataset[ dataset[attribute] == val ])\n",
        "            weight = subset_size / overall\n",
        "            props = dataset[ dataset[attribute] == val ][target].value_counts( normalize=True )\n",
        "            for p in props.array:\n",
        "              entropy =  entropy - weight*(p*math.log2(p))\n",
        "        else: #classification was chose and Gini was chosen\n",
        "          entropy = 0\n",
        "          left = dataset[ dataset[attribute] <= val ][ [attribute,target] ]\n",
        "          props = left[ target ].value_counts( normalize = True )\n",
        "          weight = len( left ) / overall\n",
        "          for prop in props.array:\n",
        "            entropy = entropy - weight*prop*math.log2( prop )\n",
        "          right = dataset[ dataset[attribute] > val ][ [attribute,target] ]\n",
        "          props = right[ target ].value_counts( normalize = True )\n",
        "          weight = len( right ) / overall\n",
        "          for prop in props.array:\n",
        "            entropy = entropy - weight*prop*math.log2( prop )\n",
        "          entropys.append(entropy)\n",
        "      else:\n",
        "        entropy = 1 - sum([p ** 2 for p in props])\n",
        "        entropy += weight * entropy\n",
        "        entropys.append(entropy)\n",
        "    minEnt=min(entropys)\n",
        "    ind=entropys.index(minEnt)\n",
        "    return features[ind]\n",
        "  else: # method is Naive Bayes Classifier/ regressor\n",
        "    #separate by class\n",
        "    separated = {}\n",
        "    for i in range(len(dataset)):\n",
        "      vector = dataset[i]\n",
        "      class_value = vector[-1]\n",
        "      if (class_value not in separated):\n",
        "        separated[class_value] = []\n",
        "      separated[class_value].append(vector)\n",
        "\n",
        "    #Summarize statistivc\n",
        "    summaries = {}\n",
        "    for class_value, instances in separated.items():\n",
        "      summaries[class_value] = [(mean(col), stdev(col), len(col)) for col in zip(*instances)]\n",
        "\n",
        "    predictions = []\n",
        "    #put test into the method\n",
        "    for row in test:\n",
        "        total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "        probabilities = {}\n",
        "        for class_value, class_summaries in summaries.items():\n",
        "          probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
        "          for i in range(len(class_summaries)):\n",
        "            mean_val, stdev_val, _ = class_summaries[i]\n",
        "            exponent = math.exp(-((row[i]-mean_val)**2 / (2 * stdev_val**2 )))\n",
        "            probabilities[class_value] *=(1 / (math.sqrt(2 * math.pi) * stdev_val)) * exponent\n",
        "        #determine the best label\n",
        "        best_label, best_prob = None, -1\n",
        "        for class_value, probability in probabilities.items():\n",
        "          if best_label is None or probability > best_prob:\n",
        "            best_prob = probability\n",
        "            best_label = class_value\n",
        "\n",
        "        predictions.append(best_label)\n",
        "    return(predictions)"
      ],
      "metadata": {
        "id": "ef6qrqhx0CcX"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}